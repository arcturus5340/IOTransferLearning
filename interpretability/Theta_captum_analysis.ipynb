{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b2a3175-d8e0-4c08-b768-eea5b4bb4950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients, DeepLift\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.utils import resample\n",
    "from sklearn import cluster\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "\n",
    "from wquantiles import quantile_1D\n",
    "\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pdb\n",
    "\n",
    "class ConfigStruct:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da59e23f-0106-45bc-996e-a549b297453d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9108eb67-ee1b-4d60-91cc-b0a7ce4dd4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    epochs=100,\n",
    "    batch_size=256, #2048\n",
    "    learning_rate=0.008, #0.008\n",
    "    weight_decay=1e-5,\n",
    "    dropout=0.05,\n",
    "    shuffle=True,\n",
    "    test_size=0.2,\n",
    "    split_seed=42,\n",
    "    random_seed=1234,\n",
    "    top10_apps_filter=False,\n",
    "    only_duplicates=False,\n",
    "    meancount75_filter=False,\n",
    "    starttime_filter=False,\n",
    "    isolation_forest_test=False,\n",
    "    feature_agglomeration=False,\n",
    "    feature_agglomeration_nclusters=64,\n",
    "    stratified_split=False,\n",
    "    smooth_l1_loss_beta=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02428d52-5484-4419-9c5c-44b03914f782",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigStruct(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5f66d1b-84c7-41af-a126-77688267ce05",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILENAME = \"Model_D_(finetuned)\"\n",
    "MODEL_DIR = r\"../models/\"\n",
    "MODEL_PATH = Path(MODEL_DIR, MODEL_FILENAME).with_suffix(\".pth\")\n",
    "\n",
    "DATASET_DIR = r\"../data/\"\n",
    "DATASET_NAME = \"theta_posix_with_apps_no_negative_outliers_no_time_witherrors\"\n",
    "DATASET_PATH = Path(DATASET_DIR, DATASET_NAME).with_suffix(\".csv\")\n",
    "\n",
    "PICKLE_DIR = r\"../models/pickle\"\n",
    "FEATUREAGGLO_NAME = r\"Model_D_(finetuned)_featureagglomeration\"\n",
    "FEATUREAGGLO_PATH = Path(PICKLE_DIR, FEATUREAGGLO_NAME).with_suffix(\".pkl\")\n",
    "ROBUSTSCALER_NAME = r\"Model_D_(finetuned)_robustscaler\"\n",
    "ROBUSTSCALER_PATH = Path(PICKLE_DIR, ROBUSTSCALER_NAME).with_suffix(\".pkl\")\n",
    "ISOLATIONFOREST_NAME = r\"Model_D_(finetuned)_isolationforest\"\n",
    "ISOLATIONFOREST_PATH = Path(PICKLE_DIR, ISOLATIONFOREST_NAME).with_suffix(\".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae16fd7a-af0d-4e1f-88f3-8e9c6a404cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>POSIX_OPENS</th>\n",
       "      <th>POSIX_FILENOS</th>\n",
       "      <th>POSIX_DUPS</th>\n",
       "      <th>POSIX_READS</th>\n",
       "      <th>POSIX_WRITES</th>\n",
       "      <th>POSIX_SEEKS</th>\n",
       "      <th>POSIX_STATS</th>\n",
       "      <th>POSIX_MMAPS</th>\n",
       "      <th>POSIX_FSYNCS</th>\n",
       "      <th>...</th>\n",
       "      <th>WRITE_10M_100M</th>\n",
       "      <th>WRITE_100M_1G</th>\n",
       "      <th>WRITE_1G_PLUS</th>\n",
       "      <th>rank</th>\n",
       "      <th>POSIX_TOTAL_TIME</th>\n",
       "      <th>nprocs</th>\n",
       "      <th>lustre</th>\n",
       "      <th>exe</th>\n",
       "      <th>mean</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7891771</td>\n",
       "      <td>7861736</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>424661</td>\n",
       "      <td>60035</td>\n",
       "      <td>90055</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.684507</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>cp2k.psmp</td>\n",
       "      <td>31.913841</td>\n",
       "      <td>-2.229334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>194</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>1499</td>\n",
       "      <td>6</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.155456</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>pw.x</td>\n",
       "      <td>11.403251</td>\n",
       "      <td>16.752206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>46037</td>\n",
       "      <td>40869</td>\n",
       "      <td>0</td>\n",
       "      <td>4713059</td>\n",
       "      <td>1719073</td>\n",
       "      <td>1271774</td>\n",
       "      <td>5429</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>71229.030892</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>train.x-2.0.3-ifort_intelmpi</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>194</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>1492</td>\n",
       "      <td>6</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.707640</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>pw.x</td>\n",
       "      <td>6.519022</td>\n",
       "      <td>-4.811382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7891771</td>\n",
       "      <td>7861736</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>424661</td>\n",
       "      <td>60035</td>\n",
       "      <td>90055</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34.010366</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>cp2k.psmp</td>\n",
       "      <td>33.631730</td>\n",
       "      <td>0.378636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 95 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  POSIX_OPENS  POSIX_FILENOS  POSIX_DUPS  POSIX_READS  POSIX_WRITES  \\\n",
       "0      0      7891771        7861736           0            3        424661   \n",
       "1      1          194            172           0           34          1499   \n",
       "2      2        46037          40869           0      4713059       1719073   \n",
       "3      3          194            172           0           34          1492   \n",
       "4      4      7891771        7861736           0            3        424661   \n",
       "\n",
       "   POSIX_SEEKS  POSIX_STATS  POSIX_MMAPS  POSIX_FSYNCS  ...  WRITE_10M_100M  \\\n",
       "0        60035        90055            0             0  ...               0   \n",
       "1            6           54            0             0  ...               0   \n",
       "2      1271774         5429            0             0  ...               0   \n",
       "3            6           54            0             0  ...               0   \n",
       "4        60035        90055            0             0  ...               0   \n",
       "\n",
       "   WRITE_100M_1G  WRITE_1G_PLUS  rank  POSIX_TOTAL_TIME  nprocs  lustre  \\\n",
       "0              0              0     0         29.684507      64       1   \n",
       "1              0              0     0         28.155456      16       1   \n",
       "2              0              0    -1      71229.030892     128       1   \n",
       "3              0              0     2          1.707640      16       1   \n",
       "4              0              0     0         34.010366      64       1   \n",
       "\n",
       "                            exe       mean      error  \n",
       "0                     cp2k.psmp  31.913841  -2.229334  \n",
       "1                          pw.x  11.403251  16.752206  \n",
       "2  train.x-2.0.3-ifort_intelmpi        NaN   0.000000  \n",
       "3                          pw.x   6.519022  -4.811382  \n",
       "4                     cp2k.psmp  33.631730   0.378636  \n",
       "\n",
       "[5 rows x 95 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "df_theta_posix = pd.read_csv(DATASET_PATH)\n",
    "df_theta_posix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93800581-49eb-4983-884b-5239ccff5f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.only_duplicates:\n",
    "    df_theta_posix = df_theta_posix[df_theta_posix[\"mean\"].isna() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0aa314eb-94cd-4b17-bdd5-bd43c455f958",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.top10_apps_filter:\n",
    "    apps_count_series = df_theta_posix.groupby(by=[\"app\"]).count()[\"nprocs\"].sort_values(ascending=False)\n",
    "    df_theta_posix = df_theta_posix[df_theta_posix.app.isin(apps_count_series[0:10].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb1be1d3-55f6-48f8-918b-cc4e53563175",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.starttime_filter:\n",
    "    df_theta_posix = df_theta_posix[df_theta_posix.start_time_sec < df_theta_posix.start_time_sec.quantile(0.25)]\n",
    "    df_theta_posix = df_theta_posix.drop([\"start_time_sec\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc6a2473-8a80-42af-b2c8-61fb44878342",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.meancount75_filter:\n",
    "    mean_counts = df_theta_posix.groupby(\"mean\",dropna=False)[\"mean\"].transform(\"count\")\n",
    "    mean_counts_quantile = pd.Series(mean_counts.unique()).quantile(0.75)\n",
    "    df_theta_posix = df_theta_posix[df_theta_posix.index.isin(mean_counts[mean_counts > mean_counts_quantile].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2162e6a-2699-4033-984b-8559e914a831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POSIX_OPENS</th>\n",
       "      <th>POSIX_FILENOS</th>\n",
       "      <th>POSIX_DUPS</th>\n",
       "      <th>POSIX_READS</th>\n",
       "      <th>POSIX_WRITES</th>\n",
       "      <th>POSIX_SEEKS</th>\n",
       "      <th>POSIX_STATS</th>\n",
       "      <th>POSIX_MMAPS</th>\n",
       "      <th>POSIX_FSYNCS</th>\n",
       "      <th>POSIX_RENAME_SOURCES</th>\n",
       "      <th>...</th>\n",
       "      <th>WRITE_1M_4M</th>\n",
       "      <th>WRITE_4M_10M</th>\n",
       "      <th>WRITE_10M_100M</th>\n",
       "      <th>WRITE_100M_1G</th>\n",
       "      <th>WRITE_1G_PLUS</th>\n",
       "      <th>rank</th>\n",
       "      <th>POSIX_TOTAL_TIME</th>\n",
       "      <th>nprocs</th>\n",
       "      <th>mean</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7891771</td>\n",
       "      <td>7861736</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>424661</td>\n",
       "      <td>60035</td>\n",
       "      <td>90055</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29.684507</td>\n",
       "      <td>64</td>\n",
       "      <td>31.913841</td>\n",
       "      <td>-2.229334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>194</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>1499</td>\n",
       "      <td>6</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.155456</td>\n",
       "      <td>16</td>\n",
       "      <td>11.403251</td>\n",
       "      <td>16.752206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46037</td>\n",
       "      <td>40869</td>\n",
       "      <td>0</td>\n",
       "      <td>4713059</td>\n",
       "      <td>1719073</td>\n",
       "      <td>1271774</td>\n",
       "      <td>5429</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>71229.030892</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>194</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>1492</td>\n",
       "      <td>6</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.707640</td>\n",
       "      <td>16</td>\n",
       "      <td>6.519022</td>\n",
       "      <td>-4.811382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7891771</td>\n",
       "      <td>7861736</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>424661</td>\n",
       "      <td>60035</td>\n",
       "      <td>90055</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34.010366</td>\n",
       "      <td>64</td>\n",
       "      <td>33.631730</td>\n",
       "      <td>0.378636</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 92 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   POSIX_OPENS  POSIX_FILENOS  POSIX_DUPS  POSIX_READS  POSIX_WRITES  \\\n",
       "0      7891771        7861736           0            3        424661   \n",
       "1          194            172           0           34          1499   \n",
       "2        46037          40869           0      4713059       1719073   \n",
       "3          194            172           0           34          1492   \n",
       "4      7891771        7861736           0            3        424661   \n",
       "\n",
       "   POSIX_SEEKS  POSIX_STATS  POSIX_MMAPS  POSIX_FSYNCS  POSIX_RENAME_SOURCES  \\\n",
       "0        60035        90055            0             0                     0   \n",
       "1            6           54            0             0                     0   \n",
       "2      1271774         5429            0             0                     0   \n",
       "3            6           54            0             0                     0   \n",
       "4        60035        90055            0             0                     0   \n",
       "\n",
       "   ...  WRITE_1M_4M  WRITE_4M_10M  WRITE_10M_100M  WRITE_100M_1G  \\\n",
       "0  ...            0             0               0              0   \n",
       "1  ...            0             0               0              0   \n",
       "2  ...          500             0               0              0   \n",
       "3  ...            0             0               0              0   \n",
       "4  ...            0             0               0              0   \n",
       "\n",
       "   WRITE_1G_PLUS  rank  POSIX_TOTAL_TIME  nprocs       mean      error  \n",
       "0              0     0         29.684507      64  31.913841  -2.229334  \n",
       "1              0     0         28.155456      16  11.403251  16.752206  \n",
       "2              0    -1      71229.030892     128        NaN   0.000000  \n",
       "3              0     2          1.707640      16   6.519022  -4.811382  \n",
       "4              0     0         34.010366      64  33.631730   0.378636  \n",
       "\n",
       "[5 rows x 92 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop column with application names\n",
    "df_theta_posix = df_theta_posix.drop(['exe','index','lustre'],axis=1)\n",
    "df_theta_posix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a51104da-436a-4c37-8d8c-1bad2711e5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       29.684507\n",
       "1       28.155456\n",
       "2    71229.030892\n",
       "3        1.707640\n",
       "4       34.010366\n",
       "Name: POSIX_TOTAL_TIME, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate bandwidth from input features\n",
    "POSIX_TOTAL_TIME_df = df_theta_posix.pop('POSIX_TOTAL_TIME')\n",
    "POSIX_TOTAL_TIME_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85d98a6f-6a74-40e4-9b51-1675b7f9c26e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>POSIX_OPENS</th>\n",
       "      <th>POSIX_FILENOS</th>\n",
       "      <th>POSIX_DUPS</th>\n",
       "      <th>POSIX_READS</th>\n",
       "      <th>POSIX_WRITES</th>\n",
       "      <th>POSIX_SEEKS</th>\n",
       "      <th>POSIX_STATS</th>\n",
       "      <th>POSIX_MMAPS</th>\n",
       "      <th>POSIX_FSYNCS</th>\n",
       "      <th>POSIX_RENAME_SOURCES</th>\n",
       "      <th>...</th>\n",
       "      <th>WRITE_1K_10K</th>\n",
       "      <th>WRITE_10K_100K</th>\n",
       "      <th>WRITE_100K_1M</th>\n",
       "      <th>WRITE_1M_4M</th>\n",
       "      <th>WRITE_4M_10M</th>\n",
       "      <th>WRITE_10M_100M</th>\n",
       "      <th>WRITE_100M_1G</th>\n",
       "      <th>WRITE_1G_PLUS</th>\n",
       "      <th>rank</th>\n",
       "      <th>nprocs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7891771</td>\n",
       "      <td>7861736</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>424661</td>\n",
       "      <td>60035</td>\n",
       "      <td>90055</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>194</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>1499</td>\n",
       "      <td>6</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46037</td>\n",
       "      <td>40869</td>\n",
       "      <td>0</td>\n",
       "      <td>4713059</td>\n",
       "      <td>1719073</td>\n",
       "      <td>1271774</td>\n",
       "      <td>5429</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3546</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>194</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>1492</td>\n",
       "      <td>6</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7891771</td>\n",
       "      <td>7861736</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>424661</td>\n",
       "      <td>60035</td>\n",
       "      <td>90055</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 89 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   POSIX_OPENS  POSIX_FILENOS  POSIX_DUPS  POSIX_READS  POSIX_WRITES  \\\n",
       "0      7891771        7861736           0            3        424661   \n",
       "1          194            172           0           34          1499   \n",
       "2        46037          40869           0      4713059       1719073   \n",
       "3          194            172           0           34          1492   \n",
       "4      7891771        7861736           0            3        424661   \n",
       "\n",
       "   POSIX_SEEKS  POSIX_STATS  POSIX_MMAPS  POSIX_FSYNCS  POSIX_RENAME_SOURCES  \\\n",
       "0        60035        90055            0             0                     0   \n",
       "1            6           54            0             0                     0   \n",
       "2      1271774         5429            0             0                     0   \n",
       "3            6           54            0             0                     0   \n",
       "4        60035        90055            0             0                     0   \n",
       "\n",
       "   ...  WRITE_1K_10K  WRITE_10K_100K  WRITE_100K_1M  WRITE_1M_4M  \\\n",
       "0  ...             0               0              0            0   \n",
       "1  ...             0               0              0            0   \n",
       "2  ...          3546               0              0          500   \n",
       "3  ...             0               0              0            0   \n",
       "4  ...             0               0              0            0   \n",
       "\n",
       "   WRITE_4M_10M  WRITE_10M_100M  WRITE_100M_1G  WRITE_1G_PLUS  rank  nprocs  \n",
       "0             0               0              0              0     0      64  \n",
       "1             0               0              0              0     0      16  \n",
       "2             0               0              0              0    -1     128  \n",
       "3             0               0              0              0     2      16  \n",
       "4             0               0              0              0     0      64  \n",
       "\n",
       "[5 rows x 89 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate duplicate set mean from input features and drop errors\n",
    "dup_set_means_series = df_theta_posix.pop('mean')\n",
    "df_theta_posix = df_theta_posix.drop([\"error\"],axis=1)\n",
    "df_theta_posix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ba12076-8f93-4f06-aa78-1c273e4cd9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix seeds for reproducibility\n",
    "random.seed(config.random_seed)\n",
    "np.random.seed(config.random_seed)\n",
    "\n",
    "torch.manual_seed(config.random_seed)\n",
    "torch.cuda.manual_seed_all(config.random_seed)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22a9cf07-f27c-4af6-b96a-bfbda9dc004e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.isolation_forest_test:\n",
    "    df_theta_posix = df_theta_posix.reset_index()[outlier_labels == 1].drop([\"index\"],axis=1)\n",
    "    POSIX_TOTAL_TIME_df = POSIX_TOTAL_TIME_df.reset_index()[outlier_labels == 1].drop([\"index\"],axis=1)\n",
    "    dup_set_means_series = dup_set_means_series.reset_index()[outlier_labels == 1].drop([\"index\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "843447ed-e676-4b96-820f-fbd11da0c7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(FEATUREAGGLO_PATH,'rb') as f:\n",
    "    agglo = pickle.load(f)\n",
    "if config.feature_agglomeration:\n",
    "    theta_posix = agglo.transform(df_theta_posix)\n",
    "else:\n",
    "    theta_posix = df_theta_posix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cebcefc-48ec-4ca1-8bcf-eec34e3f8ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.07328932e+01,  1.07072829e+01,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [-1.56821409e+03, -1.82439169e+03,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00, -4.80000000e+01],\n",
       "       [-1.55904182e+03, -1.81489192e+03,  0.00000000e+00, ...,\n",
       "         0.00000000e+00, -1.00000000e+00,  6.40000000e+01],\n",
       "       ...,\n",
       "       [-1.56130752e+03, -1.81646825e+03,  0.00000000e+00, ...,\n",
       "         0.00000000e+00, -1.00000000e+00,  3.20000000e+01],\n",
       "       [-1.56823930e+03, -1.82443697e+03, -2.20000000e+01, ...,\n",
       "         0.00000000e+00, -1.00000000e+00, -4.80000000e+01],\n",
       "       [-1.56823609e+03, -1.82443721e+03, -2.30000000e+01, ...,\n",
       "         0.00000000e+00, -1.00000000e+00, -4.80000000e+01]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale the input features\n",
    "with open(ROBUSTSCALER_PATH,'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "theta_posix_scaled = scaler.transform(theta_posix)\n",
    "theta_posix_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1403a384-372d-498d-8f8c-8d8746ab9b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_X = torch.Tensor(theta_posix_scaled).to(device)\n",
    "tensor_y = torch.Tensor(POSIX_TOTAL_TIME_df.values).view(-1, 1).to(device)\n",
    "\n",
    "test_dataset = TensorDataset(tensor_X, tensor_y)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a72fde7-5268-4b57-9c00-7e54cd1a3852",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(config.feature_agglomeration_nclusters if config.feature_agglomeration else 89, 512),\n",
    "    nn.Dropout(p=config.dropout),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 256),\n",
    "    nn.Dropout(p=config.dropout),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.Dropout(p=config.dropout),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 1)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ef30e8c-0054-4c1c-b1a4-dfd253708e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=89, out_features=512, bias=True)\n",
       "  (1): Dropout(p=0.05, inplace=False)\n",
       "  (2): ReLU()\n",
       "  (3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (4): Dropout(p=0.05, inplace=False)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (7): Dropout(p=0.05, inplace=False)\n",
       "  (8): ReLU()\n",
       "  (9): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(MODEL_PATH, map_location=torch.device(device))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19914031-378c-4507-bc60-250394ba3f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower = 0\n",
    "stride = 30000\n",
    "upper = stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7740063-b9ae-4988-a299-ac0cdb6a0a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ig_attr_annotated_full = pd.DataFrame([])\n",
    "df_dl_attr_annotated_full = pd.DataFrame([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e89883ea-b45d-49eb-af0e-2dea8e6e02db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = IntegratedGradients(model)\n",
    "deep_lift = DeepLift(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "618dfbcf-12dc-4d96-9aa7-e84c62ee4cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = df_theta_posix\n",
    "test_data = test_data.reset_index()\n",
    "test_data.to_csv(f\"./captum/Theta_captum_test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c04ec1e9-5283-4e47-81dc-a9d7a79868ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower 0 to upper 30000\n",
      "torch.Size([30000, 89])\n",
      "Integrated Gradients\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.86 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 3.44 GiB is allocated by PyTorch, and 15.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m ex \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(tensor_X[lower:upper],(upper\u001b[38;5;241m-\u001b[39mlower,config\u001b[38;5;241m.\u001b[39mfeature_agglomeration_nclusters \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mfeature_agglomeration \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m89\u001b[39m))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIntegrated Gradients\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m ig_attr \u001b[38;5;241m=\u001b[39m \u001b[43mig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m df_ig_attr_annotated_curr \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(ig_attr\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(), columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(df_theta_posix\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[0;32m     10\u001b[0m df_ig_attr_annotated_full \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df_ig_attr_annotated_full,df_ig_attr_annotated_curr])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\captum\\log\\dummy_log.py:39\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# pyre-fixme[53]: Captured variable `func` is not annotated.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# pyre-fixme[3]: Return type must be annotated.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\captum\\attr\\_core\\integrated_gradients.py:289\u001b[0m, in \u001b[0;36mIntegratedGradients.attribute\u001b[1;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta)\u001b[0m\n\u001b[0;32m    277\u001b[0m     attributions \u001b[38;5;241m=\u001b[39m _batch_attribution(\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    279\u001b[0m         num_examples,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    286\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    287\u001b[0m     )\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 289\u001b[0m     attributions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attribute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatted_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbaselines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatted_baselines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_convergence_delta:\n\u001b[0;32m    299\u001b[0m     start_point, end_point \u001b[38;5;241m=\u001b[39m baselines, inputs\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\captum\\attr\\_core\\integrated_gradients.py:368\u001b[0m, in \u001b[0;36mIntegratedGradients._attribute\u001b[1;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, step_sizes_and_alphas)\u001b[0m\n\u001b[0;32m    365\u001b[0m expanded_target \u001b[38;5;241m=\u001b[39m _expand_target(target, n_steps)\n\u001b[0;32m    367\u001b[0m \u001b[38;5;66;03m# grads: dim -> (bsz * #steps x inputs[0].shape[1:], ...)\u001b[39;00m\n\u001b[1;32m--> 368\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaled_features_tpl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_ind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_additional_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;66;03m# flattening grads so that we can multilpy it with step-size\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# calling contiguous to avoid `memory whole` problems\u001b[39;00m\n\u001b[0;32m    377\u001b[0m scaled_grads \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    378\u001b[0m     grad\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(n_steps, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(step_sizes)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mview(n_steps, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(grad\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m grad \u001b[38;5;129;01min\u001b[39;00m grads\n\u001b[0;32m    381\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\captum\\_utils\\gradient.py:128\u001b[0m, in \u001b[0;36mcompute_gradients\u001b[1;34m(forward_fn, inputs, target_ind, additional_forward_args)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03mComputes gradients of the output with respect to inputs for an\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03marbitrary forward function.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m                arguments) if no additional arguments are required\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;66;03m# runs forward pass\u001b[39;00m\n\u001b[1;32m--> 128\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43m_run_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;66;03m# _run_forward may return future of Tensor,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;66;03m# but we don't support it here now\u001b[39;00m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;66;03m# And it will fail before here.\u001b[39;00m\n\u001b[0;32m    132\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m cast(Tensor, outputs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\captum\\_utils\\common.py:588\u001b[0m, in \u001b[0;36m_run_forward\u001b[1;34m(forward_func, inputs, target, additional_forward_args)\u001b[0m\n\u001b[0;32m    585\u001b[0m inputs \u001b[38;5;241m=\u001b[39m _format_inputs(inputs)\n\u001b[0;32m    586\u001b[0m additional_forward_args \u001b[38;5;241m=\u001b[39m _format_additional_forward_args(additional_forward_args)\n\u001b[1;32m--> 588\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mforward_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# pyre-fixme[60]: Concatenation not yet support for multiple variadic\u001b[39;49;00m\n\u001b[0;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m#  tuples: `*inputs, *additional_forward_args`.\u001b[39;49;00m\n\u001b[0;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, torch\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39mthen(\u001b[38;5;28;01mlambda\u001b[39;00m x: _select_targets(x\u001b[38;5;241m.\u001b[39mvalue(), target))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\activation.py:133\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\functional.py:1704\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1702\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1704\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.86 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 3.44 GiB is allocated by PyTorch, and 15.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "while lower < len(df_theta_posix):\n",
    "    print(f\"lower {lower} to upper {upper}\")\n",
    "    print(tensor_X[lower:upper].shape)\n",
    "    ex = torch.reshape(tensor_X[lower:upper],(upper-lower,config.feature_agglomeration_nclusters if config.feature_agglomeration else 89))\n",
    "\n",
    "    print(\"Integrated Gradients\")\n",
    "    \n",
    "    ig_attr = ig.attribute(ex, n_steps=50)\n",
    "    df_ig_attr_annotated_curr = pd.DataFrame(ig_attr.cpu().detach().numpy(), columns = list(df_theta_posix.columns))\n",
    "    df_ig_attr_annotated_full = pd.concat([df_ig_attr_annotated_full,df_ig_attr_annotated_curr])\n",
    "\n",
    "    print(\"Deep Lift\")\n",
    "    deep_lift_attr = deep_lift.attribute(ex)\n",
    "    df_dl_attr_annotated_curr = pd.DataFrame(deep_lift_attr.cpu().detach().numpy(), columns = list(df_theta_posix.columns))\n",
    "    df_dl_attr_annotated_full = pd.concat([df_dl_attr_annotated_full,df_dl_attr_annotated_curr])\n",
    "    \n",
    "    lower += stride\n",
    "    upper += min(stride,len(df_theta_posix) - upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6a6169-bc3f-493b-86b2-c956f38b8d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ig_attr_annotated_full.reset_index().drop([\"index\"],axis=1).to_csv(f\"./captum/Theta_captum_ig_result.csv\")\n",
    "df_dl_attr_annotated_full.reset_index().drop([\"index\"],axis=1).to_csv(f\"./captum/Theta_captum_dl_result.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
