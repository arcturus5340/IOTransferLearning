{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab32deaa-5ae8-4d5b-872a-beb85ed88d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from ray import tune, init\n",
    "from ray.tune import Checkpoint, get_checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import ray.cloudpickle as pickle\n",
    "from ray.tune import run\n",
    "\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5968ebee-7d26-410d-8cc2-322999ee880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = os.path.abspath(\"../models\")\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, \"SmoothL1Loss_fixed_Adamax_fewer_neurons_0.2_testSize_new_StandardScaler_2048_batch_0.05_dropout_pytorch_v1.12_raytunetest.tar\")\n",
    "\n",
    "DATASET_DIR = os.path.abspath(\"../data\")\n",
    "DATASET_PATH = os.path.join(DATASET_DIR, \"blue_waters_posix_with_paths_no_negative_outliers_no_time_no_dups.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bcf78dc-0b15-453d-8615-4bb49d21cdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc0428f9-296d-42b2-be44-93d383bbf134",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"epochs\":100,\n",
    "    \"batch_size\":tune.choice([512,1024,2048,4096]),\n",
    "    \"learning_rate\":tune.loguniform(1e-4,1e-1),\n",
    "    \"l1\":tune.choice([2 ** i for i in range(7,12)]),\n",
    "    \"l2\":tune.choice([2 ** i for i in range(6,11)]),\n",
    "    \"l3\":tune.choice([2 ** i for i in range(5,9)]),\n",
    "    \"weight_decay\":tune.choice([ 1 / (10 ** i) for i in range(4,7)]), #1e-5\n",
    "    #\"dropout\":tune.choice([ 1 / (5 * (10 ** i))]), #0.05\n",
    "    \"shuffle\":True,\n",
    "    \"test_size\":0.2,\n",
    "    \"split_seed\":42,\n",
    "    \"random_seed\":1234,\n",
    "    \"stratified_split\":False,\n",
    "    \"smooth_l1_loss_beta\":1,\n",
    "    \"model_path\":MODEL_PATH,\n",
    "    \"device\":device,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a642cc92-9066-4778-992e-ee1eeceb06f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, l1=2048, l2=512, l3=128):\n",
    "        super(Net, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(90, l1),\n",
    "            #nn.Dropout(p=config[\"dropout\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l1, l2),\n",
    "            #nn.Dropout(p=config[\"dropout\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l2, l3),\n",
    "            #nn.Dropout(p=config[\"dropout\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(l3, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df00faa0-a74f-4001-98b9-0419961d59a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(config[\"random_seed\"])\n",
    "torch.cuda.manual_seed_all(config[\"random_seed\"])\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2639924f-b794-44c9-9646-ff1f9b939021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Fix seeds for reproducibility\n",
    "    random.seed(config[\"random_seed\"])\n",
    "    np.random.seed(config[\"random_seed\"])\n",
    "\n",
    "    # Load the data\n",
    "    df_blue_waters_posix = pd.read_csv(DATASET_PATH)\n",
    "\n",
    "    # Drop column with application names\n",
    "    df_blue_waters_posix = df_blue_waters_posix.drop(['path', 'exe'],axis=1)\n",
    "    \n",
    "    # Separate bandwidth from input features\n",
    "    POSIX_TOTAL_TIME_df = df_blue_waters_posix.pop('POSIX_TOTAL_TIME')\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_blue_waters_posix,\n",
    "                                                        POSIX_TOTAL_TIME_df,\n",
    "                                                        test_size=config[\"test_size\"],\n",
    "                                                        random_state=config[\"split_seed\"],\n",
    "                                                        stratify=df_blue_waters_posix[\"nprocs\"] if config[\"stratified_split\"] else None)\n",
    "\n",
    "    # Scale the input features\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    tensor_X_train = torch.Tensor(X_train_scaled)\n",
    "    tensor_y_train = torch.Tensor(y_train.values).view(-1, 1)\n",
    "    training_dataset = TensorDataset(tensor_X_train, tensor_y_train)\n",
    "    \n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    tensor_X_test = torch.Tensor(X_test_scaled)\n",
    "    tensor_y_test = torch.Tensor(y_test.values).view(-1, 1)\n",
    "    test_dataset = TensorDataset(tensor_X_test, tensor_y_test)\n",
    "\n",
    "    return training_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "602368d7-8bb3-489a-96c2-55593114bc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bluewaters_train(config):\n",
    "    model = Net().to(config[\"device\"])\n",
    "    # By default Pytorch returns avg loss per minibatch elements. But since the last batch\n",
    "    # (both in training and test) does not have enough instances, sum all the loss across the batches\n",
    "    # and then divide it by total number of elements in the the test set.\n",
    "    loss_fn = nn.SmoothL1Loss(beta=config[\"smooth_l1_loss_beta\"], reduction=\"sum\").to(config[\"device\"])\n",
    "    optimizer = optim.Adamax(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min')    \n",
    "    \n",
    "    model_epoch = 0 \n",
    "    checkpoint = get_checkpoint()\n",
    "    if checkpoint:\n",
    "        with checkpoint.as_directory() as checkpoint_dir:\n",
    "            data_path = Path(config[\"model_path\"]).with_suffix(\".pkl\")\n",
    "            with open(data_path, \"rb\") as fp:\n",
    "                checkpoint_state = pickle.load(fp)\n",
    "            model_epoch = checkpoint_state[\"epoch\"]\n",
    "            model.load_state_dict(checkpoint_state[\"net_state_dict\"])\n",
    "            optimizer.load_state_dict(checkpoint_state[\"optimizer_state_dict\"])\n",
    "            \n",
    "            print(f\"Current epoch: {model_epoch}\")\n",
    "    \n",
    "    generator = torch.Generator().manual_seed(config[\"split_seed\"])\n",
    "\n",
    "    training_dataset, test_dataset = load_data()\n",
    "    training_dataloader = DataLoader(training_dataset, batch_size=config[\"batch_size\"], shuffle=config[\"shuffle\"])\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=config[\"batch_size\"])\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(model_epoch, config[\"epochs\"]):\n",
    "        for (X, y) in training_dataloader:\n",
    "            X, y = X.to(config[\"device\"]), y.to(config[\"device\"])  # Move batch to GPU\n",
    "            y_pred = model(X)\n",
    "            \n",
    "            # Divide the summed loss by the number of elements in the current batch to get the average loss\n",
    "            loss = loss_fn(y, y_pred) / len(X)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "        model.train()\n",
    "\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X, y in test_dataloader:\n",
    "                X, y = X.to(config[\"device\"]), y.to(config[\"device\"])\n",
    "                pred = model(X)\n",
    "                test_loss += loss_fn(pred, y).item() \n",
    "\n",
    "        # Divide the summed test loss by the number of elements in the whole test dataset to get the average loss\n",
    "        test_loss /= len(test_dataloader.dataset)\n",
    "\n",
    "        #print(f\"Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "        scheduler.step(test_loss)\n",
    "\n",
    "        model_epoch = epoch  \n",
    "        checkpoint_data = {\n",
    "            \"epoch\": model_epoch,\n",
    "            \"net_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict()\n",
    "        }\n",
    "        with tempfile.TemporaryDirectory() as checkpoint_dir:\n",
    "            data_path = Path(config[\"model_path\"]).with_suffix(\".pkl\")\n",
    "            with open(data_path, \"wb\") as fp:\n",
    "                pickle.dump(checkpoint_data, fp)\n",
    "\n",
    "            checkpoint = Checkpoint.from_directory(MODEL_DIR)\n",
    "            tune.report(\n",
    "                {\"loss\": test_loss},\n",
    "                checkpoint=checkpoint,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35c0e1d3-4ae1-48fb-b629-2602feb690bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ASHAScheduler(\n",
    "    metric=\"loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=10,\n",
    "    grace_period=1,\n",
    "    reduction_factor=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d38775d3-e365-4e5d-b6ad-8f95974a286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def short_name_creator(trial):\n",
    "    config_str = json.dumps(trial.config, sort_keys=True)\n",
    "    config_hash = hashlib.sha1(config_str.encode()).hexdigest()\n",
    "    return f\"trial_{config_hash[:6]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "812ad507-a3eb-4dd8-b14e-4bf0bd8652cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 14:36:14,138\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-06-12 14:47:20</td></tr>\n",
       "<tr><td>Running for: </td><td>00:11:06.12        </td></tr>\n",
       "<tr><td>Memory:      </td><td>18.6/31.7 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=10<br>Bracket: Iter 8.000: -8957.289023989724 | Iter 4.000: -9467.410355340866 | Iter 2.000: -11127.623135067168 | Iter 1.000: -14958.214773995536<br>Logical resource usage: 10.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  l1</th><th style=\"text-align: right;\">  l2</th><th style=\"text-align: right;\">  l3</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  weight_decay</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>bluewaters_train_d402a_00000</td><td>TERMINATED</td><td>127.0.0.1:3212 </td><td style=\"text-align: right;\">         512</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">    0.000164508</td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         79.655 </td><td style=\"text-align: right;\">15135.1 </td></tr>\n",
       "<tr><td>bluewaters_train_d402a_00001</td><td>TERMINATED</td><td>127.0.0.1:3064 </td><td style=\"text-align: right;\">        2048</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">    0.00188418 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         77.2103</td><td style=\"text-align: right;\">11135.2 </td></tr>\n",
       "<tr><td>bluewaters_train_d402a_00002</td><td>TERMINATED</td><td>127.0.0.1:17060</td><td style=\"text-align: right;\">         512</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">    0.00951405 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         74.6304</td><td style=\"text-align: right;\"> 7070.28</td></tr>\n",
       "<tr><td>bluewaters_train_d402a_00003</td><td>TERMINATED</td><td>127.0.0.1:2100 </td><td style=\"text-align: right;\">        4096</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">    0.00556988 </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         78.6289</td><td style=\"text-align: right;\"> 7630.19</td></tr>\n",
       "<tr><td>bluewaters_train_d402a_00004</td><td>TERMINATED</td><td>127.0.0.1:11964</td><td style=\"text-align: right;\">        4096</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">    0.000916052</td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         14.217 </td><td style=\"text-align: right;\">15307.1 </td></tr>\n",
       "<tr><td>bluewaters_train_d402a_00005</td><td>TERMINATED</td><td>127.0.0.1:8968 </td><td style=\"text-align: right;\">        1024</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">    0.00256064 </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         57.4662</td><td style=\"text-align: right;\">10867.4 </td></tr>\n",
       "<tr><td>bluewaters_train_d402a_00006</td><td>TERMINATED</td><td>127.0.0.1:14156</td><td style=\"text-align: right;\">        2048</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\">    0.000156338</td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         13.3562</td><td style=\"text-align: right;\">17103.6 </td></tr>\n",
       "<tr><td>bluewaters_train_d402a_00007</td><td>TERMINATED</td><td>127.0.0.1:20272</td><td style=\"text-align: right;\">         512</td><td style=\"text-align: right;\">1024</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">    0.00971734 </td><td style=\"text-align: right;\">        1e-05 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         77.0125</td><td style=\"text-align: right;\"> 7321.48</td></tr>\n",
       "<tr><td>bluewaters_train_d402a_00008</td><td>TERMINATED</td><td>127.0.0.1:9056 </td><td style=\"text-align: right;\">         512</td><td style=\"text-align: right;\">2048</td><td style=\"text-align: right;\">  64</td><td style=\"text-align: right;\"> 128</td><td style=\"text-align: right;\">    0.010837   </td><td style=\"text-align: right;\">        1e-06 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         75.7935</td><td style=\"text-align: right;\"> 7056.64</td></tr>\n",
       "<tr><td>bluewaters_train_d402a_00009</td><td>TERMINATED</td><td>127.0.0.1:5108 </td><td style=\"text-align: right;\">        4096</td><td style=\"text-align: right;\"> 512</td><td style=\"text-align: right;\"> 256</td><td style=\"text-align: right;\">  32</td><td style=\"text-align: right;\">    0.0591053  </td><td style=\"text-align: right;\">        0.0001</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         79.017 </td><td style=\"text-align: right;\"> 7353.73</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th style=\"text-align: right;\">    loss</th><th>should_checkpoint  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>bluewaters_train_d402a_00000</td><td style=\"text-align: right;\">15135.1 </td><td>True               </td></tr>\n",
       "<tr><td>bluewaters_train_d402a_00001</td><td style=\"text-align: right;\">11135.2 </td><td>True               </td></tr>\n",
       "<tr><td>bluewaters_train_d402a_00002</td><td style=\"text-align: right;\"> 7070.28</td><td>True               </td></tr>\n",
       "<tr><td>bluewaters_train_d402a_00003</td><td style=\"text-align: right;\"> 7630.19</td><td>True               </td></tr>\n",
       "<tr><td>bluewaters_train_d402a_00004</td><td style=\"text-align: right;\">15307.1 </td><td>True               </td></tr>\n",
       "<tr><td>bluewaters_train_d402a_00005</td><td style=\"text-align: right;\">10867.4 </td><td>True               </td></tr>\n",
       "<tr><td>bluewaters_train_d402a_00006</td><td style=\"text-align: right;\">17103.6 </td><td>True               </td></tr>\n",
       "<tr><td>bluewaters_train_d402a_00007</td><td style=\"text-align: right;\"> 7321.48</td><td>True               </td></tr>\n",
       "<tr><td>bluewaters_train_d402a_00008</td><td style=\"text-align: right;\"> 7056.64</td><td>True               </td></tr>\n",
       "<tr><td>bluewaters_train_d402a_00009</td><td style=\"text-align: right;\"> 7353.73</td><td>True               </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-12 14:47:20,286\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to 'D:/Projects/IOTransferLearning/optimization/bluewaters_train_2025-06-12_14-36-14' in 0.0130s.\n",
      "2025-06-12 14:47:20,302\tINFO tune.py:1041 -- Total run time: 666.16 seconds (666.11 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "result = tune.run(\n",
    "    bluewaters_train,\n",
    "    resources_per_trial={\"cpu\": 10, \"gpu\": 1},\n",
    "    config=config,\n",
    "    num_samples=10,\n",
    "    scheduler=scheduler,\n",
    "    trial_dirname_creator=short_name_creator,\n",
    "    storage_path=\"file:///\" + os.path.abspath(\".\"),\n",
    "    raise_on_failed_trial=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4932ec70-5ef3-4a8c-9329-debe0c942fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial config: {'epochs': 100, 'batch_size': 512, 'learning_rate': 0.01083700425039336, 'l1': 2048, 'l2': 64, 'l3': 128, 'weight_decay': 1e-06, 'shuffle': True, 'test_size': 0.2, 'split_seed': 42, 'random_seed': 1234, 'stratified_split': False, 'smooth_l1_loss_beta': 1, 'model_path': 'D:\\\\Projects\\\\IOTransferLearning\\\\models\\\\SmoothL1Loss_fixed_Adamax_fewer_neurons_0.2_testSize_new_StandardScaler_2048_batch_0.05_dropout_pytorch_v1.12_raytunetest.tar', 'device': 'cuda'}\n",
      "Best trial final validation loss: 7056.639858972459\n"
     ]
    }
   ],
   "source": [
    "best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
    "print(f\"Best trial config: {best_trial.config}\")\n",
    "print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e99e2f-b525-4f07-ba4e-218b99427c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
